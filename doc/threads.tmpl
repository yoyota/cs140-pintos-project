            +--------------------+
            |        CS 140      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.


---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.


1 struct timer {
   2     int64_t expires;
   3     struct thread *thread;
   4     struct list_elem elem;
   5 };

   * Purpose: Represents a sleeping thread, storing its wake-up time (expires) and a pointer to the thread itself for waking it up.

  File: `devices/timer.c`



   1 static int64_t ticks;

   * Purpose: A global counter for the number of timer interrupts since the OS booted, used to measure time.



   1 static struct list timer_list;

   * Purpose: A global list containing struct timer elements for all threads that are currently sleeping via timer_sleep().


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

1. The calling thread disables interrupts, creates a timer structure containing its desired wake-up time, and adds it to a global
    list of sleeping threads.
2. The thread then blocks, yielding the CPU.
3. The periodic timer interrupt handler checks the list of sleeping threads. When a thread's wake-up time arrives, the handler
    unblocks it.
4. Once unblocked, the thread becomes ready to run and will eventually be scheduled to resume execution right after its call to
    timer_sleep().


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Here's the sequence of events and why a race condition is
  avoided:

1. A thread enters the timer_sleep() function.
2. It immediately calls enum intr_level old_level = intr_disable();. At this exact moment, all interrupts, including timer
    interrupts, are turned off for the current CPU core.
3. The code then proceeds to create a timer and add it to the timer_list.
4. Finally, it calls thread_block().


Because interrupts are disabled, a timer interrupt cannot occur between the call to intr_disable() and the point where the thread
blocks and the scheduler is invoked. The operations on the shared timer_list are performed within a critical section protected by
the interrupt disable/enable mechanism.


If a hardware timer interrupt signal arrives while interrupts are disabled, the CPU's interrupt controller will hold it in a
pending state. The interrupt will only be serviced after the current thread blocks and the scheduler switches to a new thread,
which will eventually re-enable interrupts as part of its normal operation. By that time, the sleeping thread's timer is already
safely on the timer_list, so there is no race condition.



>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Disable Interrupts: The most crucial step is the call to intr_disable() in timer_sleep(). This is done after the timer has been
added to the sleep queue but before the thread is put to sleep. Disabling interrupts prevents the CPU from responding to any
hardware interrupts, including the timer interrupt.
Atomic Operation: Because interrupts are disabled, the timer_interrupt() handler cannot run between the intr_disable() call and
the thread_block() call. This makes the process of adding a timer, disabling interrupts, and going to sleep an atomic operation
with respect to the interrupt handler.
thread_block(): The thread_block() function, which actually puts the thread to sleep, requires that interrupts are already
disabled. It even includes an ASSERT to enforce this, crashing the kernel if a thread tries to block without first disabling
interrupts.
Scheduler: thread_block() then calls schedule(), which also runs with interrupts disabled. The scheduler chooses the next thread
to run and switches to it. Interrupts will be re-enabled later, either by the new thread's context or when the original thread is
eventually woken up and timer_sleep() calls intr_set_level(old_level).

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

The current design maintains a single list of sleeping threads, always kept sorted by their wake-up time. When timer_sleep() is
called, it traverses the list to insert the new timer in its correct sorted position (an O(N) operation). This makes the timer
interrupt handler highly efficient, as it only needs to check timers at the head of the list (an O(1) operation for each expired
timer).


An alternative, more advanced design I considered is using a min-heap data structure instead of a sorted list. A min-heap would
store timers based on their wake-up time, with the timer that will expire soonest always at the root. This would offer O(log N)
performance for both adding a new timer and for removing the next expiring timer.


The sorted-list design is superior to the min-heap in the context of Pintos. While the min-heap is asymptotically faster, the
sorted list provides a much better balance of performance and simplicity for a teaching OS. It leverages the existing,
easy-to-understand list library, avoiding the need to implement a more complex heap data structure. Given that the number of
sleeping threads in Pintos is typically small, the performance gain from a min-heap would be negligible, and the added
implementation complexity would detract from the primary educational goals.


             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

`struct thread` member:
* int priority_before_donated;
    * Purpose: Stores the thread's original priority. This is used to restore a thread's priority after it no longer needs to be
        donated a higher one.
* int recent_cpu;
    * Purpose: Stores a weighted average of the CPU time a thread has used recently, for the MLFQS. It helps determine thread
        priority.
* int nice;
    * Purpose: A user-adjustable value that influences the thread's dynamic priority in the MLFQS. A lower nice value means a
        higher priority.
* struct list lock_list;
    * Purpose: A list of locks currently held by the thread. This is essential for implementing priority donation, allowing for
        nested donations.
* struct lock *lock_wait;
    * Purpose: A pointer to the lock that the thread is currently waiting to acquire. This is used to establish the donation
        chain.

Global/Static Variables:
* static struct list ready_list[PRI_MAX + 1];
    * Purpose: An array of ready lists, one for each priority level. This makes finding the highest-priority ready thread a
        constant-time operation.
* int ready_list_size;
    * Purpose: Tracks the total number of threads across all ready lists. Used for calculating the system load average in the
        MLFQS.
* int load_avg;
    * Purpose: A global variable representing the system load average, used by the MLFQS to dynamically adjust thread priorities
        based on system-wide activity.
* bool thread_mlfqs;
    * Purpose: A boolean flag, controlled by a kernel command-line option, to select between the default round-robin scheduler and
        the multi-level feedback queue scheduler.


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

The priority donation system is tracked through a combination of modifications to the struct thread and struct lock.


1. `struct thread`:
    * lock_wait (struct lock *): A pointer that is non-NULL if the thread is blocked waiting for a lock. This creates the
        dependency link: Thread -> waits for -> Lock.
    * lock_list (struct list): A list of all locks the thread currently holds. This allows a thread to be a dependency for
        multiple other threads.
    * priority_before_donated (int): Stores the thread's base priority, so when donations are no longer needed, its priority can
        be correctly restored.


2. `struct lock`:
    * holder (struct thread *): A pointer to the thread that currently owns the lock. This creates the reverse link: Lock -> held 
        by -> Thread.
    * priority (int): This field is crucial. It stores the maximum priority of all threads currently waiting in the lock's
        semaphore waiters list. When a high-priority thread starts waiting, this field is updated, signaling that a donation may be
        required.


When a thread T_H (high priority) tries to acquire a lock held by T_L (low priority), T_H is added to the lock's waiters list.
The system updates the lock's priority to T_H's priority. Then, the lock holder T_L has its effective priority elevated to match
the lock's new, higher priority. This change can propagate recursively if T_L is itself waiting on another lock.

ASCII Art Diagram of Nested Donation


This diagram shows a scenario with three threads (High, Medium, Low) and two locks (A, B), resulting in a nested priority
donation.


Initial State:
* Thread L (Priority 30) holds Lock A.
* Thread M (Priority 40) holds Lock B.
* Thread H (Priority 50) is running.

1 [Thread H (pri=50)]      [Thread M (pri=40)]      [Thread L (pri=30)]
2      |                        |                        |
3    (running)                holds                    holds
4                               |                        |
5                               V                        V
6                            (Lock B)                 (Lock A)


---

Step 1: Medium waits for Low
* Thread M attempts to acquire Lock A, but it is held by L.
* M blocks, waiting for A.
* M donates its priority (40) to L. L's effective priority becomes 40.

1 [Thread H (pri=50)]      [Thread M (pri=40)] --waits for-> (Lock A)
2      |                        |                          ^
3    (running)                holds                        |
4                               |                        donates to
5                               V                          |
6                            (Lock B)                 [Thread L (pri=40)]
7                                                          ^
8                                                          |
9                                                       (now 40, was 30)


---

Step 2: High waits for Medium (Nested Donation)
* Thread H attempts to acquire Lock B, but it is held by M.
* H blocks, waiting for B.
* H donates its priority (50) to M. M's effective priority becomes 50.
* Crucially, because M's priority was boosted while it is waiting for Lock A, it recursively donates its new, higher priority (50) 
    to Lock A's holder, L.
* L's effective priority is now 50.

1 [Thread H (pri=50)] --waits for-> (Lock B) --held by-- [Thread M (pri=50)] --waits for-> (Lock A) --held by-- [Thread L (pri=50)]
2       |                                                    ^      |                                                 ^
    
3       |                                                    |      |                                                 |
    
4       +-------------------donates to-----------------------+      |                                                 |
    
5                                                                   +------------------re-donates to -----------------+
    
6                                                                                    

  Final State:
   * H (pri=50) waits for M.
   * M (pri=50) waits for L.
   * L (pri=50) can now run with the highest priority to release Lock A, resolving the dependency chain.

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:


>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
