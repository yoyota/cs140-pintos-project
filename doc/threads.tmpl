            +--------------------+
            |        CS 140      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.


---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.


1 struct timer {
   2     int64_t expires;
   3     struct thread *thread;
   4     struct list_elem elem;
   5 };

   * Purpose: Represents a sleeping thread, storing its wake-up time (expires) and a pointer to the thread itself for waking it up.

  File: `devices/timer.c`



   1 static int64_t ticks;

   * Purpose: A global counter for the number of timer interrupts since the OS booted, used to measure time.



   1 static struct list timer_list;

   * Purpose: A global list containing struct timer elements for all threads that are currently sleeping via timer_sleep().


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

1. The calling thread disables interrupts, creates a timer structure containing its desired wake-up time, and adds it to a global
    list of sleeping threads.
2. The thread then blocks, yielding the CPU.
3. The periodic timer interrupt handler checks the list of sleeping threads. When a thread's wake-up time arrives, the handler
    unblocks it.
4. Once unblocked, the thread becomes ready to run and will eventually be scheduled to resume execution right after its call to
    timer_sleep().


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Here's the sequence of events and why a race condition is
  avoided:

1. A thread enters the timer_sleep() function.
2. It immediately calls enum intr_level old_level = intr_disable();. At this exact moment, all interrupts, including timer
    interrupts, are turned off for the current CPU core.
3. The code then proceeds to create a timer and add it to the timer_list.
4. Finally, it calls thread_block().


Because interrupts are disabled, a timer interrupt cannot occur between the call to intr_disable() and the point where the thread
blocks and the scheduler is invoked. The operations on the shared timer_list are performed within a critical section protected by
the interrupt disable/enable mechanism.


If a hardware timer interrupt signal arrives while interrupts are disabled, the CPU's interrupt controller will hold it in a
pending state. The interrupt will only be serviced after the current thread blocks and the scheduler switches to a new thread,
which will eventually re-enable interrupts as part of its normal operation. By that time, the sleeping thread's timer is already
safely on the timer_list, so there is no race condition.



>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Disable Interrupts: The most crucial step is the call to intr_disable() in timer_sleep(). This is done after the timer has been
added to the sleep queue but before the thread is put to sleep. Disabling interrupts prevents the CPU from responding to any
hardware interrupts, including the timer interrupt.
Atomic Operation: Because interrupts are disabled, the timer_interrupt() handler cannot run between the intr_disable() call and
the thread_block() call. This makes the process of adding a timer, disabling interrupts, and going to sleep an atomic operation
with respect to the interrupt handler.
thread_block(): The thread_block() function, which actually puts the thread to sleep, requires that interrupts are already
disabled. It even includes an ASSERT to enforce this, crashing the kernel if a thread tries to block without first disabling
interrupts.
Scheduler: thread_block() then calls schedule(), which also runs with interrupts disabled. The scheduler chooses the next thread
to run and switches to it. Interrupts will be re-enabled later, either by the new thread's context or when the original thread is
eventually woken up and timer_sleep() calls intr_set_level(old_level).

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

The current design maintains a single list of sleeping threads, always kept sorted by their wake-up time. When timer_sleep() is
called, it traverses the list to insert the new timer in its correct sorted position (an O(N) operation). This makes the timer
interrupt handler highly efficient, as it only needs to check timers at the head of the list (an O(1) operation for each expired
timer).


An alternative, more advanced design I considered is using a min-heap data structure instead of a sorted list. A min-heap would
store timers based on their wake-up time, with the timer that will expire soonest always at the root. This would offer O(log N)
performance for both adding a new timer and for removing the next expiring timer.


The sorted-list design is superior to the min-heap in the context of Pintos. While the min-heap is asymptotically faster, the
sorted list provides a much better balance of performance and simplicity for a teaching OS. It leverages the existing,
easy-to-understand list library, avoiding the need to implement a more complex heap data structure. Given that the number of
sleeping threads in Pintos is typically small, the performance gain from a min-heap would be negligible, and the added
implementation complexity would detract from the primary educational goals.


             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

`struct thread` member:
* int priority_before_donated;
    * Purpose: Stores the thread's original priority. This is used to restore a thread's priority after it no longer needs to be
        donated a higher one.
* int recent_cpu;
    * Purpose: Stores a weighted average of the CPU time a thread has used recently, for the MLFQS. It helps determine thread
        priority.
* int nice;
    * Purpose: A user-adjustable value that influences the thread's dynamic priority in the MLFQS. A lower nice value means a
        higher priority.
* struct list lock_list;
    * Purpose: A list of locks currently held by the thread. This is essential for implementing priority donation, allowing for
        nested donations.
* struct lock *lock_wait;
    * Purpose: A pointer to the lock that the thread is currently waiting to acquire. This is used to establish the donation
        chain.

Global/Static Variables:
* static struct list ready_list[PRI_MAX + 1];
    * Purpose: An array of ready lists, one for each priority level. This makes finding the highest-priority ready thread a
        constant-time operation.
* int ready_list_size;
    * Purpose: Tracks the total number of threads across all ready lists. Used for calculating the system load average in the
        MLFQS.
* int load_avg;
    * Purpose: A global variable representing the system load average, used by the MLFQS to dynamically adjust thread priorities
        based on system-wide activity.
* bool thread_mlfqs;
    * Purpose: A boolean flag, controlled by a kernel command-line option, to select between the default round-robin scheduler and
        the multi-level feedback queue scheduler.


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

The priority donation system is tracked through a combination of modifications to the struct thread and struct lock.


1. `struct thread`:
    * lock_wait (struct lock *): A pointer that is non-NULL if the thread is blocked waiting for a lock. This creates the
        dependency link: Thread -> waits for -> Lock.
    * lock_list (struct list): A list of all locks the thread currently holds. This allows a thread to be a dependency for
        multiple other threads.
    * priority_before_donated (int): Stores the thread's base priority, so when donations are no longer needed, its priority can
        be correctly restored.


2. `struct lock`:
    * holder (struct thread *): A pointer to the thread that currently owns the lock. This creates the reverse link: Lock -> held 
        by -> Thread.
    * priority (int): This field is crucial. It stores the maximum priority of all threads currently waiting in the lock's
        semaphore waiters list. When a high-priority thread starts waiting, this field is updated, signaling that a donation may be
        required.


When a thread T_H (high priority) tries to acquire a lock held by T_L (low priority), T_H is added to the lock's waiters list.
The system updates the lock's priority to T_H's priority. Then, the lock holder T_L has its effective priority elevated to match
the lock's new, higher priority. This change can propagate recursively if T_L is itself waiting on another lock.

ASCII Art Diagram of Nested Donation


This diagram shows a scenario with three threads (High, Medium, Low) and two locks (A, B), resulting in a nested priority
donation.


Initial State:
* Thread L (Priority 30) holds Lock A.
* Thread M (Priority 40) holds Lock B.
* Thread H (Priority 50) is running.

1 [Thread H (pri=50)]      [Thread M (pri=40)]      [Thread L (pri=30)]
2      |                        |                        |
3    (running)                holds                    holds
4                               |                        |
5                               V                        V
6                            (Lock B)                 (Lock A)


---

Step 1: Medium waits for Low
* Thread M attempts to acquire Lock A, but it is held by L.
* M blocks, waiting for A.
* M donates its priority (40) to L. L's effective priority becomes 40.

1 [Thread H (pri=50)]      [Thread M (pri=40)] --waits for-> (Lock A)
2      |                        |                          ^
3    (running)                holds                        |
4                               |                        donates to
5                               V                          |
6                            (Lock B)                 [Thread L (pri=40)]
7                                                          ^
8                                                          |
9                                                       (now 40, was 30)


---

Step 2: High waits for Medium (Nested Donation)
* Thread H attempts to acquire Lock B, but it is held by M.
* H blocks, waiting for B.
* H donates its priority (50) to M. M's effective priority becomes 50.
* Crucially, because M's priority was boosted while it is waiting for Lock A, it recursively donates its new, higher priority (50) 
    to Lock A's holder, L.
* L's effective priority is now 50.

1 [Thread H (pri=50)] --waits for-> (Lock B) --held by-- [Thread M (pri=50)] --waits for-> (Lock A) --held by-- [Thread L (pri=50)]
2       |                                                    ^      |                                                 ^
    
3       |                                                    |      |                                                 |
    
4       +-------------------donates to-----------------------+      |                                                 |
    
5                                                                   +------------------re-donates to -----------------+
    
6                                                                                    

  Final State:
   * H (pri=50) waits for M.
   * M (pri=50) waits for L.
   * L (pri=50) can now run with the highest priority to release Lock A, resolving the dependency chain.

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

1. Unsorted Waiter Lists: When a thread has to wait for a lock, semaphore, or condition, it is added to the end of a waiters list
    associated with that resource. The list is not kept sorted by priority upon insertion. It is a simple FIFO (First-In, First-Out)
    queue in terms of arrival time.


2. Linear Scan on Wake-up: The magic happens when a resource is released. In functions like sema_up() and cond_signal(), the code
    does not simply wake up the first thread in the list. Instead, it performs a linear scan of the entire waiters list to find the
    thread with the absolute highest priority.

This is accomplished using the list_max() function, which takes a custom comparison function (compare_thread_priority or
compare_semaphore_elem_priority) as an argument.


Here is the key code snippet from sema_up():


1 if (!list_empty(&sema->waiters)) {
2     struct list_elem *e =
3         list_max(&sema->waiters, compare_thread_priority, NULL);
4     list_remove(e);
5     struct thread *t = list_entry(e, struct thread, elem);
6     thread_unblock(t);
7 }

This explicitly finds the max-priority thread, removes it from the list, and unblocks it.


In summary, the system guarantees the highest-priority thread wakes up first by iterating through all waiting threads on every 
wake-up event to select the one with the highest priority, rather than by maintaining a pre-sorted list.


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When a higher-priority thread attempts to acquire a lock held by a lower-priority thread, the following sequence occurs:


1. `lock_acquire()` called: A thread, let's call it T_High, calls lock_acquire() on a lock L.
2. Check Lock Holder: The function checks if the lock L is held. Let's say it's held by another thread, T_Low, which has a lower
    priority.
3. Priority Donation:
    * Before attempting to acquire the lock's semaphore, lock_acquire() calls try_priority_donate_nest().
    * This function compares T_High's priority with T_Low's priority.
    * Since T_High's priority is greater, T_Low's priority member is updated to match T_High's priority. This is the core
        "donation" step.
4. Waiting for Lock:
    * T_High sets its lock_wait member to point to the lock L. This is crucial for handling nested donations.
    * T_High then calls sema_down() on the lock's semaphore and enters a waiting state.
5. Scheduler Action: Because T_Low now has a higher (donated) priority, the scheduler will favor running T_Low over other,
    originally higher-priority threads. This allows T_Low to finish its critical section and release the lock.
6. `lock_release()` Called: When T_Low finishes, it calls lock_release() on L.
7. Priority Restoration:
    * Inside lock_release(), the lock is removed from T_Low's list of held locks (lock_list).
    * T_Low's priority is then recalculated. If it holds no other locks that have received donations, its priority is restored to
        its original priority_before_donated. If it does hold other locks, its priority is set to the highest priority of any thread
        waiting on the locks it still holds.
8. Wake Up Waiting Thread: lock_release() calls sema_up(), which wakes up one of the waiting threads. Since the waiters list is
    sorted by priority, T_High will be chosen, and it can now acquire the lock.

Handling Nested Donation

Nested donation occurs when a chain of threads is waiting for locks. For example, T_High waits for T_Mid, who in turn waits for
T_Low. The implementation handles this elegantly using a loop within try_priority_donate_nest().

Here's the breakdown of the nested donation logic from try_priority_donate_nest():

1. Initialization: The function starts with the lock that the current thread (cur, which is T_High in our example) wants to acquire.
2. The Loop: The while loop continues as long as there is a lock in the chain (l != NULL), the lock has a holder, and the holder's
    priority is less than the current thread's priority.
3. Donation: Inside the loop, the holder's priority is donated.
4. Chaining: The crucial step for nested donation is l = l->holder->lock_wait;. This line moves the focus of the loop "up" the
    dependency chain. The lock l becomes the lock that the previous holder is waiting for.
5. Iteration: The loop then repeats, comparing T_High's priority with the new lock holder's priority, and so on, until the end of
    the chain is reached (either a lock holder has a higher or equal priority, or a thread in the chain is not waiting for any lock,
    i.e., lock_wait is NULL).


This mechanism ensures that a high priority is propagated through the entire chain of waiting threads, effectively resolving
complex priority inversion scenarios.


>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

Let's assume T_Low is the current thread holding lock L, and T_High is a higher-priority thread that is blocked, waiting to
acquire L.

1. `lock_release()` is Called: T_Low calls lock_release(L).
2. Holder is Cleared: The lock's holder field is set to NULL, indicating it is now available.
3. Priority is Restored: The priority of T_Low is recalculated. Since it no longer holds lock L, the donated priority from T_High is
    removed. Assuming T_Low holds no other locks with donated priorities, its priority reverts to its original, lower value.
4. `sema_up()` is Called: lock_release() calls sema_up() on the lock's internal semaphore to signal that the lock is free.
5. Highest-Priority Waiter is Chosen:
    * Inside sema_up(), the code checks the semaphore's waiters list, which is not empty.
    * It calls list_max() with the compare_thread_priority function to find the waiting thread with the highest priority. In this
        case, it selects T_High.
6. `thread_unblock()` is Called: T_High is removed from the waiters list, and sema_up() calls thread_unblock() to transition T_High
    to the ready state.
7. Move to Ready List: thread_unblock() places T_High onto the ready_list. It is inserted into the queue corresponding to its high
    priority level.
8. Preemption Check:
    * thread_unblock() compares the priority of the newly awakened thread (T_High) with the priority of the currently running thread
        (T_Low).
    * Since T_High->priority is greater than T_Low->priority (which was just restored to its lower base priority), the condition is
        true.
9. CPU is Yielded: Because a higher-priority thread is now ready to run, thread_yield() is called on T_Low.
10. Scheduling:
    * thread_yield() places T_Low on the appropriate (lower-priority) ready list and calls the schedule() function.
    * The scheduler's next_thread_to_run() function scans the ready lists from highest to lowest priority and selects T_High.
11. Context Switch: A context switch occurs from T_Low to T_High. T_High resumes its execution inside lock_acquire(), successfully
    acquires the lock, and proceeds into its critical section.


---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

The race condition in thread_set_priority() exists because a thread's priority is shared data. It can be modified by
thread_set_priority() at the same time it is being read by the scheduler to make decisions. This can lead to incorrect behavior.

Here is a specific scenario:
1. A running thread T_A (priority 30) calls thread_set_priority(10) to lower its priority.
2. The line T_A->priority = 10; executes.
3. Race: A timer interrupt occurs immediately. The scheduler runs and decides which thread to execute next. It sees that T_A has a
    priority of 10 and that another thread T_B on the ready list has a priority of 20. It schedules T_B to run.
4. Later, when T_A gets to run again, it resumes inside thread_set_priority() after the point where its priority was changed. It
    might then execute the logic to check if it should yield, but the preemption decision has already been handled inconsistently by
    the scheduler.

An even more complex race, which we discussed, involves priority donation happening in the middle of a priority change, leading to
the thread failing to yield when it should have.

How a Correct Implementation Avoids It

A correct implementation must ensure that the act of changing a thread's priority and the act of checking whether to yield the
CPU are atomic. In a kernel, this is typically solved by disabling interrupts during the critical section.

By wrapping the logic in intr_disable() and intr_set_level(), we prevent the scheduler's timer interrupt from interfering, thus
avoiding the race.

Can You Use a Lock?

No, you cannot use a standard lock to avoid this specific race, for two critical reasons:


1. Interrupt Context: The scheduler is often triggered by a timer interrupt. Code running in an interrupt handler cannot acquire a
    lock that might cause the thread to block or sleep. Doing so would hang or crash the entire system.
2. Deadlock: Using locks for scheduler data structures is a classic recipe for deadlock. For example, thread_set_priority might
    acquire a priority_lock and then call thread_yield, which needs to acquire a lock on the ready list. At the same time, the
    scheduler might hold the ready list lock and need the priority_lock to inspect a thread. This circular dependency would cause a
    deadlock.


---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

This design was chosen for its balance of performance, simplicity, and correctness, making optimal use of the tools available in
Pintos.

1. Scheduler Efficiency: The cornerstone of the design is the ready_list, an array of lists indexed by priority (static struct list 
    ready_list[PRI_MAX + 1]; in thread.c). This is the most efficient way to implement a priority queue when the priorities are a
    small, fixed range of integers. It makes finding the next thread to run a constant-time O(1) operation in the best case (the
    highest priority list is non-empty) and O(k) in the worst case where k is the number of priority levels, not the number of
    threads. This is vastly superior to a single ready list that would need to be kept sorted, which would make insertions O(N) or
    O(log N) with a more complex heap structure.

2. Simplicity in Synchronization: The design deliberately keeps the "wait" path simple. When a thread blocks on a semaphore, it is
    added to the waiters list with a simple list_push_back() (sema_down() in synch.c). The complexity is shifted to the "signal"
    path (sema_up()), which performs a linear scan using list_max() to wake the highest-priority waiter. This is a superior
    trade-off for this environment compared to maintaining sorted waiter lists. A sorted-list approach would complicate the wait
    logic and offer no significant performance benefit, as the number of waiters on any single lock is typically small. The current
    implementation is clean, easy to verify, and less prone to bugs.

3. Robustness: The priority donation mechanism is both robust and elegant. The try_priority_donate_nest() function in synch.c
    correctly handles arbitrarily deep nested donations by traversing the chain of waiting locks. This ensures the priority
    inversion problem is solved completely, not just for a single level.

An alternative considered was to use a single, globally-sorted ready list and also keep all lock/semaphore waiter lists sorted by
priority. The chosen design is superior because the array of ready lists provides faster scheduling, and the unsorted waiter lists
simplify the implementation of the waiting logic without a meaningful performance penalty for the scale of this system. The
chosen design is a more pragmatic and effective engineering solution for Pintos.


              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:


>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
